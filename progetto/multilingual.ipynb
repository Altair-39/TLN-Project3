{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fzsy0yet8ezy"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "import concurrent.futures\n",
        "import logging\n",
        "import os\n",
        "from functools import partial\n",
        "from typing import Dict, List, Optional, Set, Tuple, Any\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_U9JpHe8ez1"
      },
      "outputs": [],
      "source": [
        "from dotenv import find_dotenv, load_dotenv\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S5FqR5b8ez2"
      },
      "outputs": [],
      "source": [
        "from src.babelnet import get_sense\n",
        "from src.saving import save_ambiguities, save_pseudoword"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_lemma_for_lang(\n",
        "    synsets: List[Dict[str, Any]],\n",
        "    synset_id: str,\n",
        "    lang: str\n",
        ") -> str:\n",
        "    for synset in synsets:\n",
        "        props = synset.get('properties', {})\n",
        "        sid = props.get('synsetID', {}).get('id')\n",
        "        language = props.get('language', '').upper()\n",
        "        if sid == synset_id and language == lang.upper():\n",
        "            return props.get('fullLemma') or props.get('simpleLemma') or \"N/A\"\n",
        "    return \"N/A\""
      ],
      "metadata": {
        "id": "Mm3kGIOP-N1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_B2yGA108ez2"
      },
      "outputs": [],
      "source": [
        "def setup_logging() -> None:\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3PqkflO8ez3"
      },
      "outputs": [],
      "source": [
        "def check_dotenv(dotenv_path: Optional[str]) -> None:\n",
        "    if dotenv_path:\n",
        "        load_dotenv(dotenv_path)\n",
        "        logging.info(f\"Loaded environment variables from: {dotenv_path}\")\n",
        "    else:\n",
        "        logging.error(\"No .env file found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXDczAh28ez3"
      },
      "outputs": [],
      "source": [
        "def load_word_tuples(filepath: str) -> List[Tuple[str, ...]]:\n",
        "    tuples = []\n",
        "    try:\n",
        "        with open(filepath, newline='', encoding='utf-8') as file:\n",
        "            reader = csv.reader(file)\n",
        "            for row in reader:\n",
        "                words = tuple(word.strip() for word in row if word.strip())\n",
        "                if len(words) >= 2:\n",
        "                    tuples.append(words)\n",
        "    except FileNotFoundError:\n",
        "        logging.error(f\"Input file not found: {filepath}\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error reading input file: {e}\")\n",
        "        raise\n",
        "    return tuples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY7XQkad8ez4"
      },
      "outputs": [],
      "source": [
        "def find_synset_language_dict(synsets: List[dict]) -> Dict[str, Set[str]]:\n",
        "    lang_synsets: Dict[str, Set[str]] = {}\n",
        "    for synset in synsets:\n",
        "        props = synset.get('properties', {})\n",
        "        synset_id = props.get('synsetID', {}).get('id')\n",
        "        lang = props.get('language', '').upper()\n",
        "        if synset_id and lang:\n",
        "            lang_synsets.setdefault(lang, set()).add(synset_id)\n",
        "    return lang_synsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgPDo6Bw8ez4"
      },
      "outputs": [],
      "source": [
        "def extract_lemma_for_lang(synsets: List[dict], synset_id: str, lang: str) -> str:\n",
        "    lang = lang.upper()\n",
        "    for synset in synsets:\n",
        "        props = synset.get('properties', {})\n",
        "        synset_id_prop = props.get('synsetID', {}).get('id')\n",
        "        synset_lang = props.get('language', '').upper()\n",
        "        if synset_id_prop == synset_id and synset_lang == lang:\n",
        "            lemma = props.get('lemma')\n",
        "            if lemma:\n",
        "                return lemma\n",
        "    return ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNFPey0A8ez5"
      },
      "outputs": [],
      "source": [
        "def save_pseudoword_multi(pseudoword: str, words: Tuple[str, ...], synsets: List[dict],\n",
        "                          common_synsets: Set[str], langs: List[str]) -> None:\n",
        "    save_pseudoword(pseudoword, '-'.join(words), synsets, common_synsets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AgXhd658ez6"
      },
      "outputs": [],
      "source": [
        "def process_word_tuple(words: Tuple[str, ...], langs: List[str], api_key: str\n",
        "                       ) -> Optional[dict]:\n",
        "    if len(words) != len(langs):\n",
        "        logging.error(f\"Word tuple and language list length mismatch: {words}, {langs}\")\n",
        "        return None\n",
        "    synsets = get_sense(words[0], langs, api_key)\n",
        "    if not synsets:\n",
        "        logging.warning(f\"No synsets found for words: {words}\")\n",
        "        return None\n",
        "    lang_synsets = find_synset_language_dict(synsets)\n",
        "    synsets_sets = [lang_synsets.get(lang.upper(), set()) for lang in langs]\n",
        "    if not all(synsets_sets):\n",
        "        logging.info(f\"Missing synsets for some languages in {words}\")\n",
        "    common_synsets = set.intersection(*synsets_sets) if synsets_sets else set()\n",
        "    total_synsets_count = sum(len(s) for s in synsets_sets)\n",
        "    if total_synsets_count == 0:\n",
        "        ambiguity_reduction = 0.0\n",
        "    else:\n",
        "        ambiguity_reduction = 1 - (len(common_synsets) * len(langs)\n",
        "                                   ) / total_synsets_count\n",
        "    pseudoword = '-'.join(words)\n",
        "    save_pseudoword_multi(pseudoword, words, synsets, common_synsets, langs)\n",
        "    return {\n",
        "        'pseudoword': pseudoword,\n",
        "        'ambiguity_reduction': round(ambiguity_reduction, 3)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_pseudoword(\n",
        "    en_word: str,\n",
        "    it_word: str,\n",
        "    synsets: List[Dict[str, Any]],\n",
        "    common_synsets: Set[str]\n",
        ") -> None:\n",
        "    filename = f'rsrc/pseudowords_{en_word}_{it_word}.csv'\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['pseudoword', 'en_word', 'it_word',\n",
        "                        'en_sense', 'it_sense', 'common_synset_id'])\n",
        "\n",
        "        for synset_id in common_synsets:\n",
        "            en_sense = extract_lemma_for_lang(synsets, synset_id, 'EN')\n",
        "            it_sense = extract_lemma_for_lang(synsets, synset_id, 'IT')\n",
        "            pseudoword = f\"{en_word}-{it_word}\"\n",
        "            writer.writerow([pseudoword, en_word, it_word,\n",
        "                            en_sense, it_sense, synset_id])"
      ],
      "metadata": {
        "id": "2UBut0tO-Wcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_ambiguities(\n",
        "    data: List[Dict[str, Any]],\n",
        "    filename: str = 'rsrc/ambiguity_scores.json'\n",
        ") -> None:\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        json.dump(data, file, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "RZa3i_-d-ZZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECC3Y1bF8ez7"
      },
      "outputs": [],
      "source": [
        "def plot_results(ambiguity_scores: List[dict]) -> None:\n",
        "    if not ambiguity_scores:\n",
        "        logging.warning(\"No data available for plotting.\")\n",
        "        return\n",
        "    pseudowords = [result['pseudoword'] for result in ambiguity_scores]\n",
        "    scores = [result['ambiguity_reduction'] for result in ambiguity_scores]\n",
        "    sorted_indices = np.argsort(scores)\n",
        "    sorted_pseudowords = [pseudowords[i] for i in sorted_indices]\n",
        "    sorted_scores = [scores[i] for i in sorted_indices]\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    y_pos = np.arange(len(sorted_pseudowords))\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(sorted_pseudowords)))\n",
        "    bars = plt.barh(y_pos, sorted_scores, color=colors)\n",
        "    plt.yticks(y_pos, sorted_pseudowords)\n",
        "    plt.xlabel('Ambiguity Reduction Score')\n",
        "    plt.title('Pseudoword Ambiguity Reduction Results')\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()\n",
        "        plt.text(width, bar.get_y() + bar.get_height()/2,\n",
        "                 f'{width:.2f}',\n",
        "                 ha='left', va='center')\n",
        "    plt.tight_layout()\n",
        "    plot_filename = 'ambiguity_reduction_plot.png'\n",
        "    plt.savefig(plot_filename)\n",
        "    logging.info(f\"Saved plot as {plot_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NE3KgMr8ez7"
      },
      "outputs": [],
      "source": [
        "def process_word_tuple_wrapper(words: Tuple[str, ...], langs: List[str], api_key: str\n",
        "                               ) -> Optional[dict]:\n",
        "    try:\n",
        "        return process_word_tuple(words, langs, api_key)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing {words}: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main: it set the *env* and the *logger*. It use a *pool of threads* to speed up the process and at the end save the result and the bar chart."
      ],
      "metadata": {
        "id": "YVVJjGmm-e1A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGfPmKoF8ez7"
      },
      "outputs": [],
      "source": [
        "def main() -> None:\n",
        "    setup_logging()\n",
        "    dotenv_path = find_dotenv()\n",
        "    check_dotenv(dotenv_path)\n",
        "    API_KEY = os.getenv('BABELNET_API_KEY')\n",
        "    input_file = os.getenv('WORD_PAIRS')\n",
        "    langs_env = os.getenv('LANGUAGES')\n",
        "    if not API_KEY or not input_file or not langs_env:\n",
        "        logging.error(\"Required environment variables missing\")\n",
        "        return\n",
        "    langs = [lang.strip().upper() for lang in langs_env.split(',')]\n",
        "    try:\n",
        "        word_tuples = load_word_tuples(input_file)\n",
        "        valid_tuples = [words for words in word_tuples if len(words) == len(langs)]\n",
        "        max_workers = min(4, os.cpu_count() or 1)\n",
        "        chunk_size = 50\n",
        "        ambiguity_scores = []\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            process_func = partial(process_word_tuple_wrapper,\n",
        "                                   langs=langs, api_key=API_KEY)\n",
        "            for i in range(0, len(valid_tuples), chunk_size):\n",
        "                chunk = valid_tuples[i:i + chunk_size]\n",
        "                future_to_tuple = {\n",
        "                    executor.submit(process_func, words): words\n",
        "                    for words in chunk\n",
        "                }\n",
        "                for future in concurrent.futures.as_completed(future_to_tuple):\n",
        "                    words = future_to_tuple[future]\n",
        "                    try:\n",
        "                        result = future.result()\n",
        "                        if result:\n",
        "                            ambiguity_scores.append(result)\n",
        "                            logging.info(f\"Completed {words} â†’ Score:\"\n",
        "                                         f\"{result['ambiguity_reduction']:.3f}\")\n",
        "                    except Exception as e:\n",
        "                        logging.error(f\"Error processing {words}: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Fatal error: {str(e)}\", exc_info=True)\n",
        "    finally:\n",
        "        save_ambiguities(ambiguity_scores)\n",
        "        plot_results(ambiguity_scores)\n",
        "        logging.info(f\"Processed {len(ambiguity_scores)}/{len(word_tuples)} tuples\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}